# 量化简介

## 量化与反量化

设$x$ 为一个张量（它可以是模型权重，也可以是推理的中间变量），从浮点到整型的量化过程可表示如下，
$$
x_{int} = clamp(\lfloor \frac{x}{s} \rfloor +z; q_{min}, q_{max})
$$
$q_{min}$ 与 $q_{max}$ 分别表示整型值域的范围，例如 int-8量化可以取[-128, 127] ，即 $q_{min}=-128$， $q_{max}=127$， clamp(*; a, b) 表示基于[a, b]范围的截断操作。 

【图2.1 】



而从整型到浮点的反量化过程如下，
$$
\hat{x} = s(x_{int} - z)
$$
【图2.2】



关于量化参数，有很多算法基于各种搜索或优化的算法计算其较优解，从而尽量减少量化引起的精度损失，而最直接的方法即是基于张量元素min/max ，
$$
\left\{
\begin{aligned}
s  &= \frac{max(x) - min(x)}{q_{max} - q_{min}}  \\
z  &= \lfloor -\frac{min(x)}{s}  \rfloor
\end{aligned}
\right.
$$

## 对称与非对称

相比于非对称量化，对称量化的定义是量化所映射的整型值域基于0值对称，即上述公式的zero point为0，qmax = -qmin, 从而使量化的表达形式更为简化。 

非对称量化有利于充分利用量化范围。例如 Conv+ReLU输出的激励张量，其值皆为正值，若使用对称量化，则浮点将全部映射到[0~127]范围，有一半的范围未使用，其量化精度不如非对称量化。

实际中往往选择对权重张量做对称量化，而对输入张量做非对称量化。以下是来自qualcomm 的量化白皮书中的分析，如权重和输入都选择非对称量化时，以Linear层的矩阵乘法为例，将表达式展开如下，

【图2.3 】

第一项是整型张量的乘法操作，是必须的即时操作；第三、四项的操作包含了scale， zero和整型权重的乘法，这些都是提前预知的，因而可以事先计算作为偏置加上；第二项的计算依赖Xint，是每次推理需要即时计算的，而这会造成额外算力。因而当我们将权重量化改为对称量化时，则上式简化为如下，即时计算时，只需要计算第一项的矩阵乘法，
$$
\begin{aligned}
\hat{W}\hat{x} &= s_w(W_{int})s_x(x_{int}-z_x) \\
               &= s_w s_x W_{int} x_{int}-s_w s_x z_xW_{int}
\end{aligned}
$$


而当我们再省略上式的第二项时，即是两者都是对称量化时的表达式，
$$
\begin{aligned}
\hat{W}\hat{x} &= s_w(W_{int})s_x(x_{int}) \\
               &= s_w s_x (W_{int} x_{int})
\end{aligned}
$$
对比原模型中的浮点计算$Wx$，$W_{int} x_{int}$ 是整型与整型之间的乘法，后者在Nvidia GPU上的运算速度远快于前者，这就是量化模型的推理速度大大加快的原因。



# 3 LLM 量化

## challenge in LLM quantization 

从模型表现性能的角度来讲，量化自始至终要解决的一个问题是，如何保持量化后模型的精度，即让模型的部署者觉得量化后的模型在推理效率提高的同时，还能保持原来的性能。

而不同于CNN模型或者小型的Transformer模型，大模型的矩阵乘法产生的Activation张量 通常有较多的离群值，即离分布范围较远的值，俗称outliers,  这些绝对值较大但占比较低的元素值增加了量化难度。

下图分别是Resnet18 与 Opt-13B的某层输入张量的元素值统计，$\sigma$ 表示各自分布的标准差，卷积网络输入的极大值约为$28\sigma$, 且绝对值$6\sigma$ 以外的比例在0.05%；而transformer网络输入的极大值越为$325\sigma$, 且绝对值$6\sigma$ 以外的比例在0.2%。从量化效果而言，Resnet18的int-8精度基本无损失，而Opt-13B的int-8模型已精度崩塌。 



在应对激励量化的挑战这方面，有一些方案尝试降低量化精度，如下是SmoothQuant提出的思路，

![image-20231005175540256](C:\Users\jon\AppData\Roaming\Typora\typora-user-images\image-20231005175540256.png)

在矩阵乘法中，他们通过按比例缩小输入张量X的值，而将缩小的比例补偿给权重张量W，从而在保证乘法运算的积保持不变的前提下，降低张量X的量化难度。而在实际工程中，这种量化方案引起的量化误差对大模型的推理效果仍然有比较明显的影响，即使在int-8精度量化亦如此。 

所以在目前工程部署中的实用方案，大多以weight-only的量化方案为主，即放弃activation的量化。



## GPTQ

基于MSE优化weight-only的量化，可以写出如下函数，

 $argmin_{\hat{W}} \parallel WX - \hat{W}X \parallel_2^2$。 

W 即是在Transformer 中的每个Linear层权重，如Attention 模块中的 Q, K, V 以及输出层， X表示对应的输入。离线量化的过程是逐模块（Transformer）逐层对Linear层权重做量化。

其过程细节如下

- $W\in \mathbb{R}^{K\times M}$,  $X \in \mathbb{R}^{M \times N}$, $Y = W \times X \in \mathbb{R}^{K \times N}$ 
- calibrate set： 部分数据用作推理，用于查看各层输入张量的值范围，并基于此量化
- 计算Hessian (上述优化函数对于W_hat的Hessian，而非反向传播中的Hessian)， $H= \frac{1}{N} \Sigma_{i=1}^{N} {2X X^T + \lambda I} \in \mathbb{R} ^ {M\times M}$ 。 
- act order sort （如从大到小，值范围相近的column一起做量化），基于 $diag(H)$ 对 $W$ 基于M维度作列重排（降序）, $H$ 在两个维度上重排 
- 计算H^-1 (cholesky分解)
- 



![image-20231005185844657](C:\Users\jon\AppData\Roaming\Typora\typora-user-images\image-20231005185844657.png)



## 量化的细粒度



![image-20231005185117267](C:\Users\jon\AppData\Roaming\Typora\typora-user-images\image-20231005185117267.png)

